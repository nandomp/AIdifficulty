{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extract data from models to adversarial experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcastf01/creating_adversarial_images/blob/main/extract_data_from_models_to_adversarial_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWaS35yZscq_"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X42ORZhmsV96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1867fb1-05e0-449a-e7ef-38b895cc6cdc"
      },
      "source": [
        "import os, sys, math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "if 'google.colab' in sys.modules: # Colab-only Tensorflow version selector\n",
        "  %tensorflow_version 2.x\n",
        "  \n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA9VZGpDDhPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db793a1f-7747-4b27-93ec-02108d5fedf4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jan 29 09:53:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb1AoQdJsjBH"
      },
      "source": [
        "#download dataset from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2DJGVnaysCA"
      },
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cjIBD4AyuSo"
      },
      "source": [
        "##primera forma\n",
        "pendiente:añadir los arhivos correctos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp6wqeANiEGN"
      },
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "   #URL=\" https://docs.google.com/open?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx-dOJ7rshbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "025d87e7-7904-4d0e-a000-5df3638a48eb"
      },
      "source": [
        "# %%capture\n",
        "class datasets_active:\n",
        "  imagenet_validation=False  #@param {type:\"boolean\"} \n",
        "  Cesar_image_perturbedpertBig_round1=False  #@param {type:\"boolean\"} \n",
        "  Cesar_image_perturbedpertBig_round2=False  #@param {type:\"boolean\"} \n",
        "  Cesar_image_perturbedpertBig_round3=False  #@param {type:\"boolean\"} \n",
        "  Cesar_image_perturbedpertBig_round4=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round4=False#@param {type:\"boolean\"} \n",
        "  \n",
        "  David_image_perturbedpertBig_round5_075=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round5_05=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round5_025=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round5_01=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round5_005=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round5_002=False#@param {type:\"boolean\"} \n",
        "\n",
        "  David_image_perturbedpertBig_round6_075=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round6_05=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round6_025=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round6_01=False#@param {type:\"boolean\"}\n",
        "  David_image_perturbedpertBig_round6_005=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_round6_002=False#@param {type:\"boolean\"} \n",
        "\n",
        "  David_image_perturbedpertBig_mod_3_round1_002=False#@param {type:\"boolean\"} \n",
        "  David_image_perturbedpertBig_mod_3_round2_0025=True#@param {type:\"boolean\"} \n",
        "\n",
        "\n",
        "\n",
        "destination=\"pertBig.zip\"\n",
        "def get_dictionary_file_id_and_destination(file_id,destination=destination):\n",
        "  return {\n",
        "       \"file_id\":file_id,\n",
        "       \"destination\":destination,   \n",
        "        } \n",
        "\n",
        "\n",
        "files_id_destination={\n",
        "    \"imagenet_validation\":get_dictionary_file_id_and_destination(\"1QA2HeOxusHicdbUxg54M8PG2GorB8ukG\",\n",
        "                                                                 \"ILSVRC2012_img_val.tar\"),\n",
        "    \"Cesar_image_perturbedpertBig_round1\": get_dictionary_file_id_and_destination(\"1LLMgbtz1ENbZR1GFNvKIGEudMceqM4ZV\"  ),\n",
        "    # \"Cesar_image_perturbedpertBig_round2\":get_dictionary_file_id_and_destination(\"1lKXviEtQsXA481VLO-fPs5w723d186_D\"),\n",
        "    # \"Cesar_image_perturbedpertBig_round3\":get_dictionary_file_id_and_destination(\"1LLMgbtz1ENbZR1GFNvKIGEudMceqM4ZV\"),\n",
        "    # \"Cesar_image_perturbedpertBig_round4\":get_dictionary_file_id_and_destination(\"1-00I6LoDq9EOj8OMC7RU80XPSKF6LLbE\"),\n",
        "    # \"David_image_perturbedpertBig_round4\":get_dictionary_file_id_and_destination(\"12TXiuSTKVqTgTa_xmK2akLtTGbp82vuO\"),\n",
        "    \"David_image_perturbedpertBig_round5_075\":get_dictionary_file_id_and_destination(\"10MR8Q9BZhAiLpTtO8eqkO0dTlJMU0ry5\"),\n",
        "    \"David_image_perturbedpertBig_round5_05\":get_dictionary_file_id_and_destination(\"1uDyCu2igIpITT8Isy1FW9n49sM1-oeDT\"),\n",
        "    \"David_image_perturbedpertBig_round5_025\":get_dictionary_file_id_and_destination(\"1kog15Dmyj_d6t6omQudbsA2GU3SwFalN\"),\n",
        "    \"David_image_perturbedpertBig_round5_01\":get_dictionary_file_id_and_destination(\"1l0DqIr76nJI63J7yzgxvyhyTjMIZqmMy\"),\n",
        "    \"David_image_perturbedpertBig_round5_005\":get_dictionary_file_id_and_destination(\"1IVsB5QwGt-5GfefbrvSVGFI2cGW7yIGx\"),    \n",
        "    \"David_image_perturbedpertBig_round5_002\":get_dictionary_file_id_and_destination(\"1MuJfPRm14dWzDXAV_JjEyRjZ3iZOh6Zw\"),\n",
        "    \n",
        "    \"David_image_perturbedpertBig_round6_075\":get_dictionary_file_id_and_destination(\"1__vhYkd-FpNNCIYuLSyyS2huvTOfsAZT\"),\n",
        "    \"David_image_perturbedpertBig_round6_05\":get_dictionary_file_id_and_destination(\"19szuYAsFPVA922UpOlypz_0PFX0H7IPa\"),\n",
        "    \"David_image_perturbedpertBig_round6_025\":get_dictionary_file_id_and_destination(\"1RZ-VC5hx4HV9pYrvGKQ5IRAbTbzRsiBI\"),\n",
        "    \"David_image_perturbedpertBig_round6_01\":get_dictionary_file_id_and_destination(\"1VxSr6rZvpDxVVAmMeV7najEeiAPrtq1Z\"),\n",
        "    \"David_image_perturbedpertBig_round6_005\":get_dictionary_file_id_and_destination(\"1lXkFZT7rMu2urCS3U3kwy0Oubp2yfGUc\"),    \n",
        "    \"David_image_perturbedpertBig_round6_002\":get_dictionary_file_id_and_destination(\"1x4-pVseLzzAusfUo7lFkA5PEsV28U38p\"),\n",
        "    \n",
        "    \"David_image_perturbedpertBig_mod_3_round1_002\":get_dictionary_file_id_and_destination(\"1Ce8mcLqTU9ogoByCZw65FyjJxKwCVR8j\"),\n",
        "    \"David_image_perturbedpertBig_mod_3_round2_0025\":get_dictionary_file_id_and_destination(\"1aRXJOZZf0BOUBmQEuJtY7NkK83Y-2j7H\"),\n",
        "\n",
        "    }\n",
        "    \n",
        "for dataset_name, dictionary_fileid_destination in files_id_destination.items():\n",
        "  is_active=getattr(datasets_active,dataset_name)\n",
        "  if is_active:\n",
        "    destination=dictionary_fileid_destination[\"destination\"]\n",
        "    file_id=dictionary_fileid_destination[\"file_id\"]\n",
        "    download_file_from_google_drive(file_id, destination)\n",
        "\n",
        "    !unzip -q /content/pertBig.zip\n",
        "    print(\"the dataset :\",dataset_name, \"was download and unzip\")\n",
        "\n",
        "\n",
        "label_id=\"1PKH4QWZe_VCOhu19oOhbV9z-YKrKACO7\"\n",
        "destination_label_id=\"/content/label.txt\"\n",
        "download_file_from_google_drive(label_id, destination_label_id)\n",
        "\n",
        "label_real_name=\"1sxe3eunq5U4EwsHlLaeRmwcnaZjCEcnh\"\n",
        "destination_label_real_name=\"/content/name_real_label.txt\"\n",
        "download_file_from_google_drive(label_real_name, destination_label_real_name)\n",
        "#confirmar que se descarga bien ya que si se ha descargado varias veces al ser tan grande luego no te deja"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the dataset : David_image_perturbedpertBig_mod_3_round2_0025 was download and unzip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOjFbuUiu0ip"
      },
      "source": [
        "##upload the drive to storage the information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNYy-HP9lARE"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdDkm5DwDfTP"
      },
      "source": [
        "El tiempo que se necesita en setup es del\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC8x36masw2w"
      },
      "source": [
        "#Device detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2NXBlb7syR5"
      },
      "source": [
        "#@title Detect hardware\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError:\n",
        "  tpu = None\n",
        "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "    \n",
        "# Select appropriate distribution strategy for hardware\n",
        "if tpu:\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "  print('Running on TPU ', tpu.master())  \n",
        "elif len(gpus) > 0:\n",
        "  #strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n",
        "  print('Running on ', len(gpus), ' GPU(s) ')\n",
        "  strategy=tf.distribute.OneDeviceStrategy(gpus[0])\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "\n",
        "# How many accelerators do we have ?\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ-uJzl_CHml"
      },
      "source": [
        "#Functions Globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBw11Q5ACJYs",
        "cellView": "form"
      },
      "source": [
        "#@title functions to create model\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def show_batch(image_batch, label_batch):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(25):\n",
        "      ax = plt.subplot(5,5,n+1)\n",
        "      \n",
        "      plt.imshow(image_batch[n])\n",
        "      #plt.title(real_class[(CLASS_NAMES[label_batch[n]==1][0])].title())\n",
        "      plt.title(real_class[np.argmax([label_batch[n]])].title())\n",
        "      plt.axis('off')\n",
        "def get_models_made():\n",
        "  path_base=\"/content/drive/My Drive/AI/Ejemplos adversariales/resultados2\"\n",
        "  files=os.listdir(path_base)\n",
        "  models_made= [ model[:-4].lower() for model in files]\n",
        "  return models_made\n",
        "\n",
        "def create_model(name_model,classes=1000):\n",
        "  with strategy.scope():\n",
        "\n",
        "    model=getattr(tf.keras.applications,name_model)(include_top=True, weights='imagenet', classes=classes) #quite input shape por motivos varios\n",
        "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
        "    METRICS=[ \n",
        "      \n",
        "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "      tf.keras.metrics.Accuracy(name=\"accuracy_binary\"),\n",
        "      tf.keras.metrics.TruePositives(name='tp'),\n",
        "      tf.keras.metrics.FalsePositives(name='fp'),\n",
        "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      tf.keras.metrics.AUC(name='auc',num_thresholds=500),#cambiar el auc\n",
        "      tf.keras.metrics.TopKCategoricalAccuracy( k=3, name='top_3_categorical_accuracy'),\n",
        "      tf.keras.metrics.TopKCategoricalAccuracy( k=5, name='top_5_categorical_accuracy'),\n",
        "      tf.keras.metrics.TopKCategoricalAccuracy( k=10, name='top_10_categorical_accuracy')\n",
        "        ]\n",
        "  \n",
        "    # model.compile(loss='categorical_crossentropy',\n",
        "    #               optimizer=opt,\n",
        "    #               metrics=METRICS)\n",
        "    return model\n",
        "\n",
        "    \n",
        "def train_model(model,train_data,validation_data=None,callbacks=None):\n",
        "\n",
        "  model.fit(train_data,\n",
        "    #steps_per_epoch =( train_generator.samples // BATCH_SIZE),\n",
        "    validation_data = validation_data, \n",
        "    #alidation_steps = (validation_generator.samples // BATCH_SIZE),\n",
        "    epochs = 100,\n",
        "   #callbacks=callbacks\n",
        "   )\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5NrxKzmCA0H"
      },
      "source": [
        "#Models API Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNLhwhckC5yG"
      },
      "source": [
        "##functions necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMFU8PFLpIkM"
      },
      "source": [
        "import numpy as np \n",
        "import pathlib\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v6N8rv2gvb9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Ci7yDlg1ID",
        "cellView": "form"
      },
      "source": [
        "#@title creating functions to create model and extract preprocesing input\n",
        "def clean_list_of_models_names(lists_of_private_object):\n",
        "  #todos los modelos empiezan por mayuscula y no son privados, es decir no pueden empezar con \"_\"\n",
        "  prev_clean_list=list()\n",
        "  clean_list=list()\n",
        "  for element in lists_of_private_object:\n",
        "    if element[0]==\"_\" or not element[0].isupper():\n",
        "      pass\n",
        "    else:\n",
        "      \n",
        "      prev_clean_list.append(element)\n",
        "  return prev_clean_list\n",
        "\n",
        "def clean_list_of_familys(lists_of_private_object):\n",
        "  #todos los modelos empiezan por mayuscula y no son privados, es decir no pueden empezar con \"_\"\n",
        "  prev_clean_list=list()\n",
        "  clean_list=list()\n",
        "  for element in lists_of_private_object:\n",
        "    if element[0]==\"_\" or not element[0].islower() or element==\"imagenet_utils\":\n",
        "      pass\n",
        "    else:      \n",
        "      prev_clean_list.append(element)\n",
        "  return prev_clean_list\n",
        "  \n",
        "def get_preprocesing_input(ALL_FAMILYS,model):\n",
        "  def check_which_is_family_is_the_family_to_which_it_belongs(ALL_FAMILYS,model):\n",
        "\n",
        "    for family in ALL_FAMILYS:\n",
        "      family_module=getattr(tf.keras.applications,family)\n",
        "      if model in dir(family_module):\n",
        "        return family_module\n",
        "    return \"ERROR, NO SE HA ENCONTRADO EN NINGUNA FAMILIA\"\n",
        "  def get_function_preprocess_input(module_family_net):\n",
        "    return getattr(module_family_net,\"preprocess_input\")\n",
        "  family_module=check_which_is_family_is_the_family_to_which_it_belongs(ALL_FAMILYS,model)\n",
        "  preprocess_input=(get_function_preprocess_input(family_module))\n",
        "  return preprocess_input\n",
        "\n",
        "def check_is_csv_is_created(model_name):\n",
        "  path_to_check=path_result+\"/\"+model_name+\".csv\"\n",
        "  if os.path.isfile(path_to_check): \n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def create_model(name_model,classes=1000):\n",
        "  with strategy.scope():\n",
        "\n",
        "    model=getattr(tf.keras.applications,name_model)(include_top=True, weights='imagenet', classes=classes) #quite input shape por motivos varios\n",
        "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
        "    METRICS=[ \n",
        "      \n",
        "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "      tf.keras.metrics.Accuracy(name=\"accuracy_binary\"),\n",
        "      tf.keras.metrics.TruePositives(name='tp'),\n",
        "      tf.keras.metrics.FalsePositives(name='fp'),\n",
        "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      tf.keras.metrics.AUC(name='auc',num_thresholds=500),#cambiar el auc\n",
        "      tf.keras.metrics.TopKCategoricalAccuracy( k=3, name='top_3_categorical_accuracy'),\n",
        "      tf.keras.metrics.TopKCategoricalAccuracy( k=5, name='top_5_categorical_accuracy'),\n",
        "      tf.keras.metrics.TopKCategoricalAccuracy( k=10, name='top_10_categorical_accuracy')\n",
        "        ]\n",
        "  \n",
        "    # model.compile(loss='categorical_crossentropy',\n",
        "    #               optimizer=opt,\n",
        "    #               metrics=METRICS)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8cJG8lOC5Rd",
        "cellView": "form"
      },
      "source": [
        "#@title funcionts to create list and predict\n",
        "def show_batch(image_batch, label_batch):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(25):\n",
        "      ax = plt.subplot(5,5,n+1)\n",
        "      plt.imshow(image_batch[n])\n",
        "      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
        "      plt.axis('off')\n",
        "      \n",
        "def create_list_file_ds(path):\n",
        "  Imagenet_root = pathlib.Path(path)\n",
        "  list_ds = tf.data.Dataset.list_files(str(Imagenet_root/'*/*'))\n",
        "  return list_ds\n",
        "\n",
        "def create_list_of_dataset_with_preprocess(filename):\n",
        "  \n",
        "  parts = tf.strings.split(filename, os.sep)\n",
        "  \n",
        "  label = parts[-2]\n",
        "\n",
        "  image = tf.io.read_file(filename,)\n",
        "  ################ try tf.cast como tiene cesar###\n",
        "  \n",
        "  #probar pasandolo a pil\n",
        "\n",
        "  #probar usando tf.io.decode\n",
        "  image = tf.io.decode_jpeg(image,channels=3)\n",
        "  print(image)\n",
        "  # image = tf.image.decode_jpeg(image,channels=3)\n",
        "  image = tf.image.resize(image, [IMG_WIDTH, IMG_HEIGHT])\n",
        "  filename_string=parts[-1]\n",
        "\n",
        "\n",
        "  try:\n",
        "    image=function_to_preprocess(image)\n",
        "  except:  \n",
        "    print(\"error\")\n",
        "    print(filename)\n",
        "    print(image.shape)\n",
        "    print(filename_string)\n",
        "    \n",
        "  return image,label,filename_string\n",
        "\n",
        "def predict_model(model,map_de_dataset,df,model_name):\n",
        "  #testeando el metodo lista, el anterior metodo está repetido con otro nombre más abajo\n",
        "  start_time=time.time()\n",
        "  init_batch=time.time()\n",
        "  count_batch=0\n",
        "  count_image=0\n",
        "\n",
        "  list_filename=list()\n",
        "  list_code_predict=list()\n",
        "  list_probability_class=list()\n",
        "  list_class_name_predict=list()\n",
        "  list_class_code_real=list()\n",
        "  list_class_name_real=list()\n",
        "  list_probability_class_real=list()\n",
        "  list_model=list()\n",
        "  list_id_image=list()\n",
        "  list_count=list()\n",
        "  for image_batch, label_batch,filename_batch in map_de_dataset.batch(nbatch).take(niteration):  \n",
        "    # print(\"filename: \", filename.numpy())\n",
        "    count_batch+=1\n",
        "    prediction=model.predict(image_batch.numpy())\n",
        "    for image_prediction,label,filename_like_bytes in zip(prediction,label_batch,filename_batch):\n",
        "      count_image+=1\n",
        "\n",
        "      class_code_predict=int(np.argmax(image_prediction))\n",
        "      list_code_predict.append(class_code_predict)\n",
        "      # print(\"predictionlabel: \",np.argmax(image_prediction))\n",
        "      probability_class=image_prediction[class_code_predict]\n",
        "      list_probability_class.append(probability_class)\n",
        "      # print(\"probability_class: \",probability_class)\n",
        "      class_name_predict=real_class[class_code_predict]\n",
        "      list_class_name_predict.append(class_name_predict)\n",
        "      # print(\"class_name_predict: \",class_name_predict)\n",
        "\n",
        "      class_code_real=int(label.numpy().decode(\"UTF-8\"))\n",
        "      list_class_code_real.append(class_code_real)\n",
        "      class_name_real=real_class[class_code_real]\n",
        "      list_class_name_real.append(class_name_real)\n",
        "      probability_class_real=image_prediction[class_code_real]\n",
        "      list_probability_class_real.append(probability_class_real)\n",
        "      # print(\"class_code_real: \", class_code_real)\n",
        "      # print(\"class_name_real: \",class_name_real)\n",
        "      # print(\"probability_class_real: \", probability_class_real)\n",
        "\n",
        "      filename=filename_like_bytes.numpy().decode('UTF-8')\n",
        "      list_filename.append(filename)\n",
        "      id_image=filename[-13:-5]\n",
        "      list_model.append(model_name)\n",
        "      list_id_image.append(id_image)\n",
        "      list_count.append(count_image)\n",
        "      # print(\"filename: \",filename)\n",
        "      # print(\"id_image: \",id_image)\n",
        "      \n",
        "     \n",
        "    time_batch=(time.time() - init_batch)\n",
        "    if count_batch%250==0:\n",
        "      print(\"--- %s segundos es lo que ha tardado este batch en ser predecido---\" % (time_batch)) \n",
        "      print(\"--- %s minutos es lo que tardaría en predecir todo el conjunto de imágenes---\" % (time_batch*niteration/60))  \n",
        "      print(\"--- %s batch analizado---\" % (round(count_batch/niteration*100,2))) \n",
        "  \n",
        "    init_batch=time.time()\n",
        "  time_create_dataframe=time.time()\n",
        "  df=pd.DataFrame(list(zip(list_filename,list_code_predict,list_probability_class,list_class_name_predict,\n",
        "                           list_class_code_real,list_class_name_real,list_probability_class_real,list_model,list_id_image)),\n",
        "                  columns=[\"filename\",\"code_predict\",\"probability_class\",\"class_name_predict\",\"class_code_real\",\n",
        "                           \"class_name_real\",\"probability_class_real\",\"model\",\"id_image\"],index=[list_count])\n",
        "  duration_creation_dataframe=(time.time()-time_create_dataframe)\n",
        "  print(\"--- %s segundos es lo que ha tardado en crearse el dataframe---\" % (duration_creation_dataframe)) \n",
        "  time_total=(time.time() - start_time)\n",
        "  print(\"------ %s minutos es lo que se tardo en predecir todo el conjunto de imágenes------\" % (time_total/60))  \n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zCBzBv6MDir"
      },
      "source": [
        "#study perturbed image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BVYtLlKDfZP"
      },
      "source": [
        "##setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGJJVwo4M5Da"
      },
      "source": [
        "se tienen que haber descargado los datos , luego se tiene unlog.txt con la información de las imagenes y una carpeta con las imágenes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D11C3EddNaQ8"
      },
      "source": [
        "#@title get labels\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "\n",
        "def get_dictionary_with_real_class():\n",
        "  path_json_real_label=\"/content/name_real_label.txt\" #el json es el bueno\n",
        "  real_class_prev=json.loads(open(path_json_real_label).read())\n",
        "  real_class=dict()\n",
        "  for k in real_class_prev:\n",
        "    real_class[int(k)]=real_class_prev[k][1] #dictionary key:number value:string\n",
        "  return real_class\n",
        "\n",
        "def change_key_per_value_in_dictionary(dictionary):\n",
        "  new_dict=dict()\n",
        "  for k,v in dictionary.items():\n",
        "    if v in new_dict.keys():\n",
        "      print(k)\n",
        "    new_dict[v]=k\n",
        "  return new_dict\n",
        "def get_code_name_prediction(name):\n",
        "\n",
        "  return real_class_inverted[name]\n",
        "\n",
        "real_class=get_dictionary_with_real_class()\n",
        "real_class_format_to_generator= {str(k): v for k, v in real_class.items()}\n",
        "real_class_inverted=change_key_per_value_in_dictionary(real_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mASx-hoxjdh"
      },
      "source": [
        "#@title for when we test various  experiments\r\n",
        "logs=[file_txt[:-4] for file_txt in glob.glob(\"*.txt\") if file_txt.split(\"_\")[0]==\"log\"]\r\n",
        "epsilons_available=[log.split(\"_\")[-1] for log in logs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hL_jHmk4rBX"
      },
      "source": [
        "#@title predict all images with all the models\r\n",
        "for epsilon in epsilons_available:\r\n",
        "  print(\"doing\", epsilon, \"experiment\")\r\n",
        "  data_dir_raw = \"/content/result_adversarial\"#@param {type:\"string\"}\r\n",
        "  data_dir_raw=data_dir_raw+epsilon\r\n",
        "  validation_labels_file = \"/content/label.txt\"#@param {type:\"string\"}\r\n",
        "  data_dir=\"/content/perturbed_order\"#@param {type:\"string\"}\r\n",
        "  data_dir=data_dir+epsilon\r\n",
        "  log_dir=\"/content/log_e_{}.txt\".format(epsilon)\r\n",
        "  mod1=False #@param {type:\"boolean\"}\r\n",
        "  mod3=True #@param {type:\"boolean\"}\r\n",
        "  if mod1:\r\n",
        "    data = pd.read_csv(log_dir, sep=\",\",index_col=False, header=None,\r\n",
        "                      names=[\"originalfilename\",\"original_prediction\",\"confidence_original\",\"epsilon\",\"pertubed_prediction\",\"confidence_pertubed\",\"name_new_file\"])\r\n",
        "    data[\"pertubed_code\"]=data[\"pertubed_prediction\"].apply(get_code_name_prediction)\r\n",
        "  elif mod3:\r\n",
        "    data = pd.read_csv(log_dir, sep=\",\",index_col=False, header=None,\r\n",
        "                      names=[\"model_name\",\r\n",
        "                             \"target\",\r\n",
        "                             \"originalfilename\",\r\n",
        "                             \"original_prediction\",\r\n",
        "                             \"confidence_original\",\r\n",
        "                             \"epsilon\",\r\n",
        "                             \"all_models_had_exist_creating_adversarial\",\r\n",
        "                             \"confidence_pertubed\",\r\n",
        "                             \"name_new_file\"])\r\n",
        "  \r\n",
        "\r\n",
        "  data[\"original_code\"]=data[\"original_prediction\"].apply(get_code_name_prediction)\r\n",
        "  \r\n",
        "  result_dir = pathlib.Path(data_dir_raw)\r\n",
        "  labels=data[\"original_code\"].unique().tolist()\r\n",
        "  unique_labels = set(labels)\r\n",
        "\r\n",
        "\r\n",
        "  for label in unique_labels:\r\n",
        "      labeled_data_dir = os.path.join(data_dir, str(label))\r\n",
        "      # Catch error if sub-directory exists\r\n",
        "      try:\r\n",
        "        os.makedirs(labeled_data_dir)\r\n",
        "      except OSError as e:\r\n",
        "        print(e)\r\n",
        "\r\n",
        "  for index, row in data.iterrows():\r\n",
        "    try:\r\n",
        "      perturbed_filename=row[\"name_new_file\"]\r\n",
        "      original_code=row[\"original_code\"]\r\n",
        "\r\n",
        "      perturbed_path = os.path.join(data_dir_raw, perturbed_filename)\r\n",
        "      new_filename = os.path.join(data_dir, str(original_code), perturbed_filename)\r\n",
        "      os.rename(perturbed_path, new_filename)\r\n",
        "    except OSError as e:\r\n",
        "      print(e)\r\n",
        "\r\n",
        "  ##Prediction\r\n",
        "\r\n",
        "  path_result=\"/content/drive/MyDrive/AI/Ejemplos adversariales/mod3/round2/\" #@param {type:\"string\"}\r\n",
        "  path_result=path_result+\"{}/CSV individuales\".format(\"0\"+str(epsilon.split(\".\")[-1]))\r\n",
        "\r\n",
        "  path_width_data= data_dir\r\n",
        "  AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
        "\r\n",
        "  nbatch=50\r\n",
        "  nImages=data.shape[0]\r\n",
        "  niteration=math.ceil(nImages/nbatch)\r\n",
        "\r\n",
        "\r\n",
        "  all_models_in_keras_without_clean=dir(tf.keras.applications)\r\n",
        "  ALL_MODELS=clean_list_of_models_names(all_models_in_keras_without_clean)\r\n",
        "  ALL_MODELS=ALL_MODELS[:15] +ALL_MODELS[17:]\r\n",
        "  ALL_FAMILYS=clean_list_of_familys(all_models_in_keras_without_clean)\r\n",
        "  \r\n",
        "  for model_name in ALL_MODELS:\r\n",
        "    print(model_name)\r\n",
        "    init_model_time=time.time()\r\n",
        "    if check_is_csv_is_created(model_name)==False:\r\n",
        "      preprocess_input=get_preprocesing_input(ALL_FAMILYS,model_name)\r\n",
        "      function_to_preprocess=preprocess_input\r\n",
        "      model=create_model(model_name)\r\n",
        "      IMG_WIDTH=model.input.shape[1]\r\n",
        "      IMG_HEIGHT =model.input.shape[2]\r\n",
        "      \r\n",
        "      list_ds=create_list_file_ds(path_width_data)\r\n",
        "      \r\n",
        "      labeled_ds = list_ds.map(create_list_of_dataset_with_preprocess,num_parallel_calls=AUTOTUNE)\r\n",
        "\r\n",
        "      df=pd.DataFrame()\r\n",
        "      df=predict_model(model,labeled_ds,df,model_name)\r\n",
        "      \r\n",
        "      if df.shape[0]>0:\r\n",
        "            df.to_csv(path_result+\"/\"+model_name+\"perturbed.csv\")\r\n",
        "      else:\r\n",
        "          print(\"En el modelo \"+ model_name+\" hubo un error, por lo que ver que paso\")\r\n",
        "      \r\n",
        "      del (list_ds)\r\n",
        "      del (labeled_ds)\r\n",
        "      \r\n",
        "      \r\n",
        "      tf.keras.backend.clear_session() #quizá para limpiar la ram\r\n",
        "    print(\"------ %s minutos es lo que se tardo en realizar todo el modelo------\" % ((time.time()-init_model_time)/60))  \r\n",
        "\r\n",
        "  # result_Directory=\"/content/drive/My Drive/AI/Ejemplos adversariales/resultados_api_tensorflow/prediction_models_v2\"\r\n",
        "  result_Directory=path_result\r\n",
        "  result_dir = pathlib.Path(result_Directory)\r\n",
        "  # CLASS_NAMES = np.array([item.name for item in result_dir.glob('.csv') ]\r\n",
        "  filenames=[item.name for item in result_dir.glob('*.csv') ]\r\n",
        "  path_result_join= \"/content/drive/MyDrive/AI/Ejemplos adversariales/mod3/round2/\" #@param {type:\"string\"}\r\n",
        "  path_result_join=path_result_join+\"{}\".format(\"0\"+str(epsilon.split(\".\")[-1]))\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSs13RM39JNf"
      },
      "source": [
        "##Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdeUeokwDSoO"
      },
      "source": [
        "##ánalisis de imágenes perturbadas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28g9dVGlkIRX"
      },
      "source": [
        "# # result_Directory=\"/content/drive/My Drive/AI/Ejemplos adversariales/resultados_api_tensorflow/prediction_models_v2\"\n",
        "# result_Directory=path_result\n",
        "# result_dir = pathlib.Path(result_Directory)\n",
        "# # CLASS_NAMES = np.array([item.name for item in result_dir.glob('.csv') ]\n",
        "# filenames=[item.name for item in result_dir.glob('*.csv') ]\n",
        "# # path_result_join= \"/content/drive/My Drive/AI/Ejemplos adversariales/Round 5/002\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31u2ztYXkTgr"
      },
      "source": [
        "#@title threshold\n",
        "threshold = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "df_global=pd.DataFrame()\n",
        "path_base=result_Directory+\"/\"\n",
        "for filename in filenames:\n",
        "  df_aux=pd.read_csv(path_base+filename)\n",
        "  df_aux.drop(columns=\"Unnamed: 0\",inplace=True)\n",
        "  df_aux[\"model\"]=filename[:-4]#only this time\n",
        "  df_global=pd.concat([df_global,df_aux],ignore_index=True)\n",
        "condition_class_predict_is_real=df_global[\"code_predict\"]==df_global[\"class_code_real\"]\n",
        "condition_prediction_is_more_than_threshold=df_global[\"probability_class_real\"]>=threshold\n",
        "conditions=condition_class_predict_is_real & condition_prediction_is_more_than_threshold\n",
        "df_global[\"epsilon\"]= [text[9:14] for text in df_global[\"filename\"]] #careful here with the number of digits\n",
        "df_global[\"filename\"]= [text[15:] for text in df_global[\"filename\"]]\n",
        "df_global[\"model\"]=[text[:-9] for text in df_global[\"model\"]]\n",
        "\n",
        "df_global[\"isright\"]=np.where(conditions ,1,0)\n",
        "df_global[\"ispertubed\"]=True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvmqnRCRt1QZ"
      },
      "source": [
        "Realizar un multilevel con image y epsilon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o5ZQ1x4QSEN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtdBFRiMELXH"
      },
      "source": [
        "df_global.to_csv(path_result_join+\"/perturbed_complete_df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOpoaitH1WVD"
      },
      "source": [
        "df_global.pivot_table(index=['filename', 'epsilon'],columns=\"model\",values=\"isright\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0fTOxXdD-1C"
      },
      "source": [
        "df_global.pivot_table(index=['filename', 'epsilon'],columns=\"model\",values=\"isright\").to_csv(path_result_join+\"/matriz_bool_ejemplos_adversariales_perturbadas.csv\")\n",
        "df_global.pivot_table(index=['filename', 'epsilon'],columns=\"model\",values=\"probability_class_real\").to_csv(path_result_join+\"/matriz_porcentaje_ejemplos_adversariales_perturbadas.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmKOGoD321Rm"
      },
      "source": [
        "df_global.pivot_table(index=['filename', 'epsilon'],columns=\"model\",values=\"isright\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6pXteDk2hXY"
      },
      "source": [
        "print(\"el modelo que tiene que acertar todo Mobilenet sus resultados son un accuracy de:\",df_global.pivot_table(index=['filename', 'epsilon'],columns=\"model\",values=\"isright\").MobileNet.mean())\r\n",
        "print(\"el modelo que tiene que fallar todo ResNet50V2 sus resultados son un accuracy de:\",df_global.pivot_table(index=['filename', 'epsilon'],columns=\"model\",values=\"isright\").ResNet50V2.mean())\r\n",
        "print(\"el modelo que tiene que fallar un 50% y acertar el otro 50% todo DenseNet121 sus resultados son un accuracy de:\",df_global.pivot_table(index=['filename', 'epsilon'],columns=\"model\",values=\"isright\").DenseNet121.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU4rd3Sn28PD"
      },
      "source": [
        "#@title threshold\r\n",
        "threshold = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.01}\r\n",
        "df_global=pd.DataFrame()\r\n",
        "path_base=result_Directory+\"/\"\r\n",
        "for filename in filenames:\r\n",
        "  df_aux=pd.read_csv(path_base+filename)\r\n",
        "  df_aux.drop(columns=\"Unnamed: 0\",inplace=True)\r\n",
        "  df_aux[\"model\"]=filename[:-4]#only this time\r\n",
        "  df_global=pd.concat([df_global,df_aux],ignore_index=True)\r\n",
        "condition_class_predict_is_real=df_global[\"class_code_real\"]==df_global[\"class_code_real\"]\r\n",
        "condition_prediction_is_more_than_threshold=df_global[\"probability_class\"]>=threshold\r\n",
        "conditions=condition_class_predict_is_real & condition_prediction_is_more_than_threshold\r\n",
        "df_global[\"epsilon\"]= [text[9:14] for text in df_global[\"filename\"]] #careful here with the number of digits\r\n",
        "df_global[\"filename\"]= [text[15:] for text in df_global[\"filename\"]]\r\n",
        "df_global[\"model\"]=[text[:-9] for text in df_global[\"model\"]]\r\n",
        "\r\n",
        "df_global[\"isright\"]=np.where(conditions ,1,0)\r\n",
        "df_global[\"ispertubed\"]=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fRM4k7mwtOr"
      },
      "source": [
        "df_global"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjTcSG_g3b7S"
      },
      "source": [
        "df_global.pivot_table(index=['filename', 'epsilon'],columns=\"model\",values=\"isright\").mean()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUzpbyZfP6Hn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}